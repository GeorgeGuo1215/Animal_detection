# DeepSeek-OCR å·¥ä½œæµç¨‹è¯¦è§£ä¸SAMè¾“å‡ºæˆªè·æŒ‡å—

## ğŸ“‹ ç›®å½•
1. [æ•´ä½“æ¶æ„](#æ•´ä½“æ¶æ„)
2. [ä¸ºä»€ä¹ˆèƒ½è‡ªåŠ¨ç”Ÿæˆç»“æœ](#ä¸ºä»€ä¹ˆèƒ½è‡ªåŠ¨ç”Ÿæˆç»“æœ)
3. [SAMæ¨¡å‹çš„ä½œç”¨](#samæ¨¡å‹çš„ä½œç”¨)
4. [å¦‚ä½•æˆªè·SAMè¾“å‡º](#å¦‚ä½•æˆªè·samè¾“å‡º)
5. [å®é™…æ“ä½œæ­¥éª¤](#å®é™…æ“ä½œæ­¥éª¤)

---

## ğŸ—ï¸ æ•´ä½“æ¶æ„

DeepSeek-OCR æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ–‡æ¡£ç†è§£æ¨¡å‹ï¼Œå…¶æ¶æ„åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š

```
è¾“å…¥å›¾åƒ
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. SAM æ¨¡å‹ (Segment Anything)     â”‚
â”‚     - æå–ä½å±‚è§†è§‰ç‰¹å¾              â”‚
â”‚     - è¾“å‡º: [B, 1024, H', W']       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. CLIP æ¨¡å‹                       â”‚
â”‚     - æå–é«˜å±‚è¯­ä¹‰ç‰¹å¾              â”‚
â”‚     - ä¸SAMç‰¹å¾ç»“åˆ                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Projector + Language Model      â”‚
â”‚     - å°†è§†è§‰ç‰¹å¾æŠ•å½±åˆ°æ–‡æœ¬ç©ºé—´      â”‚
â”‚     - ç”ŸæˆOCRæ–‡æœ¬å’Œä½ç½®ä¿¡æ¯         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
è¾“å‡º: Markdownæ–‡æœ¬ + ä½ç½®æ ‡æ³¨
```

---

## ğŸ¯ ä¸ºä»€ä¹ˆèƒ½è‡ªåŠ¨ç”Ÿæˆç»“æœå›¾ç‰‡å’ŒMarkdownæ–‡æ¡£ï¼Ÿ

### 1. **æ¨¡å‹çš„ `infer()` æ–¹æ³•**

å½“ä½ è°ƒç”¨ `model.infer()` æ—¶ï¼Œè¿™ä¸ªæ–¹æ³•å†…éƒ¨æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

```python
res = model.infer(
    tokenizer, 
    prompt=prompt,           # "<image>\n<|grounding|>Convert the document to markdown."
    image_file=image_file,   # è¾“å…¥å›¾åƒè·¯å¾„
    output_path=output_path, # è¾“å‡ºç›®å½•
    base_size=1024,          # å…¨å±€è§†å›¾çš„åŸºç¡€å¤§å°
    image_size=640,          # å±€éƒ¨è£å‰ªçš„å›¾åƒå¤§å°
    crop_mode=True,          # æ˜¯å¦ä½¿ç”¨è£å‰ªæ¨¡å¼
    save_results=True,       # âœ¨ å…³é”®å‚æ•°ï¼šè‡ªåŠ¨ä¿å­˜ç»“æœ
    test_compress=True
)
```

**å…³é”®ï¼š`save_results=True`** å‚æ•°ä¼šè§¦å‘ä»¥ä¸‹æ“ä½œï¼š

#### a) ç”Ÿæˆå¸¦ä½ç½®æ ‡æ³¨çš„è¾“å‡º

æ¨¡å‹è¾“å‡ºæ ¼å¼åŒ…å«ç‰¹æ®Štokenï¼š
```markdown
<ref>text content<box>[[x1,y1,x2,y2]]</box></ref>
```

ä¾‹å¦‚ï¼š
```markdown
<ref>Sample Document<box>[[100,50,500,100]]</box></ref>
<ref>This is a paragraph.<box>[[100,120,500,200]]</box></ref>
```

#### b) åå¤„ç†æµç¨‹

`infer()` æ–¹æ³•å†…éƒ¨ä¼šï¼š

1. **æå–ä½ç½®ä¿¡æ¯**ï¼šè§£æ `<ref>` å’Œ `<box>` æ ‡ç­¾
2. **ç»˜åˆ¶è¾¹ç•Œæ¡†**ï¼šåœ¨åŸå›¾ä¸Šç»˜åˆ¶æ£€æµ‹åˆ°çš„æ–‡æœ¬åŒºåŸŸ
3. **ç”ŸæˆMarkdown**ï¼šç§»é™¤ä½ç½®æ ‡æ³¨ï¼Œä¿ç•™çº¯æ–‡æœ¬
4. **ä¿å­˜æ–‡ä»¶**ï¼š
   - `result.mmd`: æ¸…ç†åçš„Markdownæ–‡æ¡£
   - `result_with_boxes.jpg`: å¸¦è¾¹ç•Œæ¡†çš„å¯è§†åŒ–å›¾åƒ

### 2. **ä»£ç å®ç°ä½ç½®**

`infer()` æ–¹æ³•çš„å®ç°åœ¨ **HuggingFace Hub** çš„æ¨¡å‹ä»“åº“ä¸­ï¼Œå› ä¸ºï¼š

```python
model = AutoModel.from_pretrained(
    model_name, 
    trust_remote_code=True,  # âœ¨ å…è®¸æ‰§è¡Œè¿œç¨‹ä»£ç 
    use_safetensors=True
)
```

`trust_remote_code=True` ä¼šä» HuggingFace ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹çš„è‡ªå®šä¹‰ä»£ç ï¼ŒåŒ…æ‹¬ `infer()` æ–¹æ³•ã€‚

### 3. **å‚è€ƒvLLMç‰ˆæœ¬çš„å®ç°**

ä»é¡¹ç›®ä¸­çš„ vLLM ç‰ˆæœ¬å¯ä»¥çœ‹åˆ°ç±»ä¼¼çš„åå¤„ç†é€»è¾‘ï¼š

```python
# DeepSeek-OCR-vllm/run_dpsk_ocr_image.py
def draw_bounding_boxes(image, refs):
    """ç»˜åˆ¶è¾¹ç•Œæ¡†"""
    for ref in refs:
        # è§£æ <ref> å’Œ <box> æ ‡ç­¾
        label_type, points_list = extract_coordinates_and_label(ref)
        # åœ¨å›¾åƒä¸Šç»˜åˆ¶æ¡†
        draw.rectangle([x1, y1, x2, y2], outline=color)
    return image

# ä¿å­˜ç»“æœ
result_image.save(f'{OUTPUT_PATH}/result_with_boxes.jpg')
with open(f'{OUTPUT_PATH}/result.mmd', 'w') as f:
    f.write(cleaned_markdown)
```

---

## ğŸ” SAMæ¨¡å‹çš„ä½œç”¨

### SAM (Segment Anything Model)

**SAM æ˜¯ä¸€ä¸ªå¼ºå¤§çš„è§†è§‰ç¼–ç å™¨ï¼Œä¸“é—¨ç”¨äºæå–å›¾åƒçš„åº•å±‚ç‰¹å¾ã€‚**

### åœ¨ DeepSeek-OCR ä¸­çš„è§’è‰²ï¼š

1. **ç‰¹å¾æå–å™¨**
   ```python
   # å¤„ç†å…¨å±€å›¾åƒ
   global_features_1 = self.sam_model(image_ori)
   # è¾“å‡º: [batch, 1024, H, W]
   
   # å¤„ç†å±€éƒ¨è£å‰ªpatches
   local_features_1 = self.sam_model(patches)
   # è¾“å‡º: [batch, 1024, H', W']
   ```

2. **ä¸CLIPç»“åˆ**
   ```python
   # SAMæä¾›ä½å±‚ç‰¹å¾
   sam_features = self.sam_model(image)
   
   # CLIPå¤„ç†é«˜å±‚è¯­ä¹‰ï¼Œæ¥æ”¶SAMç‰¹å¾ä½œä¸ºè¾“å…¥
   clip_features = self.vision_model(image, sam_features)
   
   # æ‹¼æ¥ä¸¤ç§ç‰¹å¾
   combined_features = torch.cat([clip_features[:, 1:], 
                                  sam_features.flatten(2).permute(0, 2, 1)], 
                                  dim=-1)
   
   # æŠ•å½±åˆ°è¯­è¨€æ¨¡å‹ç©ºé—´
   final_features = self.projector(combined_features)
   ```

3. **å¤šå°ºåº¦å¤„ç†**
   - **Global View**: å¤„ç†æ•´ä¸ªå›¾åƒçš„ç¼©æ”¾ç‰ˆæœ¬
   - **Local View**: å¤„ç†é«˜åˆ†è¾¨ç‡çš„å±€éƒ¨è£å‰ªå—
   - ä¸¤è€…ç»“åˆæä¾›æ›´ä¸°å¯Œçš„è§†è§‰ä¿¡æ¯

### SAMçš„è¾“å‡ºç»“æ„ï¼š

```python
# SAMæ¨¡å‹æ¶æ„
class ImageEncoderViT:
    def forward(self, x):
        x = self.patch_embed(x)        # [B, H*W, 768]
        for block in self.blocks:
            x = block(x)                # Transformer blocks
        
        x = self.neck(x)                # [B, 256, H, W]
        x = self.net_2(x)               # [B, 512, H/2, W/2]
        x = self.net_3(x)               # [B, 1024, H/4, W/4]
        return x
```

**è¾“å‡ºç‰¹å¾å›¾**ï¼š
- å½¢çŠ¶: `[batch_size, 1024, H/4, W/4]`
- æ¯ä¸ªç©ºé—´ä½ç½®çš„1024ç»´ç‰¹å¾å‘é‡
- åŒ…å«ä¸°å¯Œçš„è§†è§‰è¯­ä¹‰ä¿¡æ¯

---

## ğŸ£ å¦‚ä½•æˆªè·SAMè¾“å‡º

### æ–¹æ³•1: ä½¿ç”¨ PyTorch Hookï¼ˆæ¨èï¼‰

PyTorchçš„Hookæœºåˆ¶å…è®¸ä½ åœ¨ä¸ä¿®æ”¹æ¨¡å‹ä»£ç çš„æƒ…å†µä¸‹æ‹¦æˆªä¸­é—´å±‚çš„è¾“å‡ºã€‚

#### å®Œæ•´ä»£ç ç¤ºä¾‹ï¼š

```python
from transformers import AutoModel, AutoTokenizer
import torch
import numpy as np

# åŠ è½½æ¨¡å‹
model = AutoModel.from_pretrained(
    'deepseek-ai/DeepSeek-OCR', 
    trust_remote_code=True
).eval().cuda()

# å­˜å‚¨SAMè¾“å‡ºçš„å®¹å™¨
sam_outputs = []

def sam_hook(module, input, output):
    """
    Hookå‡½æ•°ï¼šæ¯æ¬¡SAMæ¨¡å‹å‰å‘ä¼ æ’­æ—¶è°ƒç”¨
    
    å‚æ•°:
        module: è¢«hookçš„æ¨¡å—ï¼ˆSAMæ¨¡å‹ï¼‰
        input: è¾“å…¥åˆ°æ¨¡å—çš„æ•°æ®
        output: æ¨¡å—çš„è¾“å‡º
    """
    # ä¿å­˜è¾“å‡ºï¼ˆæ³¨æ„detaché¿å…å½±å“æ¢¯åº¦ï¼‰
    sam_outputs.append(output.detach().cpu())
    
    # æ‰“å°ä¿¡æ¯
    print(f"SAM Output Shape: {output.shape}")
    print(f"SAM Output Range: [{output.min():.3f}, {output.max():.3f}]")
    
    return output

# æ³¨å†Œhook
# éœ€è¦æ‰¾åˆ°æ¨¡å‹ä¸­SAMçš„ç¡®åˆ‡è·¯å¾„
if hasattr(model, 'vision_model') and hasattr(model.vision_model, 'sam_model'):
    handle = model.vision_model.sam_model.register_forward_hook(sam_hook)
    print("âœ“ Hook registered successfully")

# è¿è¡Œæ¨ç†
res = model.infer(tokenizer, prompt=prompt, image_file=image_file, ...)

# ä¿å­˜SAMè¾“å‡º
for idx, sam_out in enumerate(sam_outputs):
    np.save(f'sam_output_{idx}.npy', sam_out.numpy())
    print(f"Saved: sam_output_{idx}.npy, shape={sam_out.shape}")

# æ¸…ç†hook
handle.remove()
```

### æ–¹æ³•2: ä¿®æ”¹æ¨¡å‹æºç 

å¦‚æœHookæ–¹æ³•ä¸workï¼Œä½ éœ€è¦ä¿®æ”¹HuggingFaceä¸‹è½½çš„æ¨¡å‹ä»£ç ã€‚

#### æ­¥éª¤ï¼š

1. **æ‰¾åˆ°ç¼“å­˜çš„æ¨¡å‹ä»£ç **ï¼š
   ```bash
   # HuggingFaceç¼“å­˜é€šå¸¸åœ¨ï¼š
   # Windows: C:\Users\<username>\.cache\huggingface\modules\transformers_modules\
   # Linux: ~/.cache/huggingface/modules/transformers_modules/
   ```

2. **å®šä½SAMè°ƒç”¨ä½ç½®**ï¼š
   åœ¨æ¨¡å‹æ–‡ä»¶ä¸­æœç´¢ `sam_model` æˆ–ç±»ä¼¼çš„è°ƒç”¨

3. **æ·»åŠ ä¿å­˜ä»£ç **ï¼š
   ```python
   # åœ¨ _pixel_values_to_embedding æˆ–ç±»ä¼¼æ–¹æ³•ä¸­
   def forward(self, ...):
       # ...
       sam_output = self.sam_model(image)
       
       # æ·»åŠ ä¿å­˜é€»è¾‘
       torch.save(sam_output, 'sam_output.pt')
       np.save('sam_output.npy', sam_output.cpu().numpy())
       
       # ...ç»§ç»­åŸæœ‰é€»è¾‘
   ```

### æ–¹æ³•3: åˆ›å»ºåŒ…è£…ç±»

```python
class SAMWrapper(nn.Module):
    def __init__(self, sam_model, save_dir='./sam_outputs'):
        super().__init__()
        self.sam_model = sam_model
        self.save_dir = save_dir
        self.counter = 0
        os.makedirs(save_dir, exist_ok=True)
    
    def forward(self, x):
        output = self.sam_model(x)
        
        # ä¿å­˜è¾“å‡º
        save_path = f"{self.save_dir}/sam_{self.counter}.npy"
        np.save(save_path, output.detach().cpu().numpy())
        self.counter += 1
        
        return output

# æ›¿æ¢åŸå§‹SAMæ¨¡å‹
model.vision_model.sam_model = SAMWrapper(model.vision_model.sam_model)
```

---

## ğŸš€ å®é™…æ“ä½œæ­¥éª¤

### Step 1: ä½¿ç”¨æä¾›çš„è„šæœ¬

æˆ‘å·²ç»ä¸ºä½ åˆ›å»ºäº† `run_dpsk_ocr_with_sam_output.py`ï¼Œç›´æ¥è¿è¡Œï¼š

```bash
cd F:\BaiduNetdiskDownload\æ¸¯åŸå¤§\deepseek-ocr\DeepSeek-OCR\DeepSeek-OCR-master\DeepSeek-OCR-hf
conda activate deepseek-ocr
python run_dpsk_ocr_with_sam_output.py
```

### Step 2: æ£€æŸ¥è¾“å‡º

è„šæœ¬ä¼šç”Ÿæˆï¼š
- `sam_output_*.npy`: SAMæ¨¡å‹çš„åŸå§‹è¾“å‡º
- `sam_feature_vis_*.png`: SAMç‰¹å¾çš„å¯è§†åŒ–ï¼ˆç¬¬ä¸€ä¸ªé€šé“ï¼‰
- `result.mmd`: OCRç»“æœçš„Markdownæ–‡æ¡£
- `result_with_boxes.jpg`: å¸¦è¾¹ç•Œæ¡†çš„å›¾åƒ

### Step 3: åˆ†æSAMè¾“å‡º

```python
import numpy as np
import matplotlib.pyplot as plt

# åŠ è½½SAMè¾“å‡º
sam_out = np.load('sam_output_0.npy')
print(f"Shape: {sam_out.shape}")  # [batch, 1024, H, W]

# å¯è§†åŒ–ä¸åŒé€šé“
fig, axes = plt.subplots(2, 4, figsize=(15, 8))
for i in range(8):
    ax = axes[i//4, i%4]
    feature_map = sam_out[0, i*128, :, :]  # æ¯éš”128ä¸ªé€šé“å–ä¸€ä¸ª
    ax.imshow(feature_map, cmap='viridis')
    ax.set_title(f'Channel {i*128}')
    ax.axis('off')
plt.tight_layout()
plt.savefig('sam_features_visualization.png')
```

### Step 4: å¦‚æœHookä¸å·¥ä½œ

å¯èƒ½éœ€è¦è°ƒè¯•æ¨¡å‹ç»“æ„ï¼š

```python
# æ‰“å°æ¨¡å‹ç»“æ„
def print_model_structure(model, prefix=''):
    for name, child in model.named_children():
        print(f"{prefix}{name}: {type(child).__name__}")
        print_model_structure(child, prefix + '  ')

print_model_structure(model)
```

æ‰¾åˆ°SAMæ¨¡å—çš„ç¡®åˆ‡è·¯å¾„åï¼Œä¿®æ”¹hookæ³¨å†Œä»£ç ã€‚

---

## ğŸ“Š é¢„æœŸè¾“å‡ºç¤ºä¾‹

### ç»ˆç«¯è¾“å‡ºï¼š
```
âœ“ Successfully registered hook to sam_model
============================================================
å¼€å§‹æ¨ç†...
============================================================
[SAM Output] Shape: torch.Size([1, 1024, 64, 64])
[SAM Output] Min: -2.3456, Max: 3.7890, Mean: 0.1234
[SAM Output] Shape: torch.Size([4, 1024, 64, 64])
[SAM Output] Min: -1.9876, Max: 4.2345, Mean: 0.2345
============================================================
æ¨ç†å®Œæˆï¼
============================================================
æ•è·åˆ° 2 ä¸ªSAMè¾“å‡º
ä¿å­˜ SAM è¾“å‡º 0: shape=(1, 1024, 64, 64) -> ../../output/sam_output_0.npy
ä¿å­˜ SAM ç‰¹å¾å¯è§†åŒ– 0 -> ../../output/sam_feature_vis_0.png
```

### æ–‡ä»¶è¾“å‡ºï¼š
```
output/
â”œâ”€â”€ result.mmd                    # OCRç»“æœMarkdown
â”œâ”€â”€ result_with_boxes.jpg         # å¸¦è¾¹ç•Œæ¡†çš„å›¾åƒ
â”œâ”€â”€ sam_output_0.npy              # SAMè¾“å‡º1
â”œâ”€â”€ sam_output_1.npy              # SAMè¾“å‡º2
â”œâ”€â”€ sam_feature_vis_0.png         # å¯è§†åŒ–1
â””â”€â”€ sam_feature_vis_1.png         # å¯è§†åŒ–2
```

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### é—®é¢˜1: "Could not find sam_model"

**åŸå› **: æ¨¡å‹ç»“æ„ä¸é¢„æœŸä¸åŒ

**è§£å†³**:
```python
# æ–¹æ³•1: æ‰“å°æ‰€æœ‰å±æ€§
print("Model attributes:", dir(model))
for attr in dir(model):
    if 'sam' in attr.lower() or 'vision' in attr.lower():
        print(f"  Found: {attr}")

# æ–¹æ³•2: é€’å½’æœç´¢
def find_module(model, target_class_name):
    for name, module in model.named_modules():
        if target_class_name in str(type(module)):
            print(f"Found {target_class_name} at: {name}")
            return name, module
    return None, None

path, sam_module = find_module(model, 'ImageEncoderViT')
```

### é—®é¢˜2: Hookæ²¡æœ‰è¢«è°ƒç”¨

**åŸå› **: å¯èƒ½SAMæ¨¡å‹åœ¨æ¨¡å‹å¤–éƒ¨è¢«è°ƒç”¨ï¼Œæˆ–è€…ä½¿ç”¨äº†ç¼–è¯‘ä¼˜åŒ–

**è§£å†³**:
- å°è¯•åœ¨æ›´ä¸Šå±‚æ³¨å†Œhook
- æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº† `torch.compile()` ç­‰ä¼˜åŒ–

### é—®é¢˜3: å†…å­˜æº¢å‡º

**åŸå› **: SAMè¾“å‡ºå¾ˆå¤§ï¼ˆ1024ä¸ªé€šé“ï¼‰

**è§£å†³**:
```python
# åªä¿å­˜éƒ¨åˆ†é€šé“
def sam_hook(module, input, output):
    # åªä¿å­˜å‰64ä¸ªé€šé“
    reduced_output = output[:, :64, :, :].detach().cpu()
    sam_outputs.append(reduced_output)
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

1. **DeepSeek-OCR è®ºæ–‡**: è¯¦ç»†ç®—æ³•åŸç†
2. **SAM (Segment Anything)**: https://segment-anything.com/
3. **PyTorch Hooksæ–‡æ¡£**: https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook
4. **é¡¹ç›®ä¸­çš„vLLMå®ç°**: `DeepSeek-OCR-vllm/deepseek_ocr.py` (ç¬¬394, 404è¡Œ)

---

## âœ… æ€»ç»“

1. **ä¸ºä»€ä¹ˆèƒ½ç”Ÿæˆç»“æœ**: `model.infer()` å†…éƒ¨å®ç°äº†å®Œæ•´çš„åå¤„ç†æµç¨‹
2. **SAMçš„ä½œç”¨**: æå–ä½å±‚è§†è§‰ç‰¹å¾ï¼Œä¸CLIPç»“åˆå½¢æˆå¤šå°ºåº¦è¡¨ç¤º
3. **å¦‚ä½•æˆªè·**: ä½¿ç”¨PyTorch Hookæœºåˆ¶æœ€ç®€å•ï¼Œæ— éœ€ä¿®æ”¹ä»£ç 
4. **å®é™…åº”ç”¨**: ä½¿ç”¨æä¾›çš„è„šæœ¬ `run_dpsk_ocr_with_sam_output.py`

ç¥ä½ å®éªŒé¡ºåˆ©ï¼ğŸ‰




















